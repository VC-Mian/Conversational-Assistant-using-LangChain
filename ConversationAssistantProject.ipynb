{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234e00da",
   "metadata": {
    "id": "234e00da"
   },
   "source": [
    "# LangChain for Conversational Assistants\n",
    "\n",
    "In this assignment, you will make use of the LangChain library to connect different tools and LLM outputs together to form a conversational system that will play the role of a personalized, conversational assistant. This assignment will consist of a sequence of steps utilized to create a meaningful LangChain structure:\n",
    "\n",
    "1. Use an LLM to handle typical conversational assistant needs.\n",
    "    - Understand if the model needs to access the external file.\n",
    "    - Summarize different information provided to the model.\n",
    "    - Converse with the user.\n",
    "2. Understand the use of components.\n",
    "    - Understand the use cases of a VectorStore database (i.e. information extraction).\n",
    "    - Understand the use cases of LLMs with different system prompts.\n",
    "3. Link systems with LangChain.\n",
    "4. Gain a greater understanding of the Huggingface Transformers library.\n",
    "    - Utilize the database to gain more information about NLP.\n",
    "\n",
    "\n",
    "In an exploration of the bonus material, you will additionally:\n",
    "\n",
    "1. Understand how to load new tools utilizing the LangChain package.\n",
    "    - Specifically, understand the use of the ArXiv search tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b57c2",
   "metadata": {
    "id": "917b57c2"
   },
   "source": [
    "## Write your name here: Vanessa Miranda-Calleja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d249c",
   "metadata": {
    "id": "319d249c"
   },
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells).\n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *ConversationalAssistants.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *ConversationalAssistants.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas.\n",
    "8. Make sure your your Canvas submission contains the correct files by downloading it after posting it on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2583be",
   "metadata": {},
   "source": [
    "Use this cell to install LangChain and the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb7b3a6",
   "metadata": {
    "id": "9cb7b3a6"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain_core\n",
    "!pip install -q langchain\n",
    "!pip install -q langchain_community\n",
    "!pip install -q lancedb\n",
    "!pip install -q langchain_openai\n",
    "!pip install -q gdown\n",
    "!pip install -q arxiv\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q optimum\n",
    "!pip install -q onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de878e0e",
   "metadata": {
    "id": "de878e0e"
   },
   "source": [
    "# Setting up the VectorStore database\n",
    "\n",
    "The code below sets up a LanceDB vector database. Utilizing the OpenAIEmbeddings class, it sets up an embedding for the system to utilize to take text and project the information into a table of vectors.\n",
    "\n",
    "Running this cell is only required for Google Colab, otherwise the file was provided in `hw04/data/lancedb_jina_code`. Since the folders are structured by the output of LanceDB, and large, it is more convenient to use this method to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nJklkH-ZRVuS",
   "metadata": {
    "id": "nJklkH-ZRVuS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/lancedb_jina_code\\\\hf_docs.lance\\\\_transactions\\\\0-2d466177-6f33-40cf-977b-8cd61ed4d4a9.txn',\n",
       " '../data/lancedb_jina_code\\\\hf_docs.lance\\\\_transactions\\\\1-60f05143-44b9-4a6b-9e3c-367f57b9ceb4.txn',\n",
       " '../data/lancedb_jina_code\\\\hf_docs.lance\\\\_transactions\\\\2-b9eaa9af-c2ee-4987-89df-c93c745ec54c.txn',\n",
       " '../data/lancedb_jina_code\\\\hf_docs.lance\\\\_versions\\\\1.manifest',\n",
       " '../data/lancedb_jina_code\\\\hf_docs.lance\\\\_versions\\\\2.manifest',\n",
       " '../data/lancedb_jina_code\\\\hf_docs.lance\\\\_versions\\\\3.manifest',\n",
       " '../data/lancedb_jina_code\\\\hf_docs.lance\\\\data\\\\df2482de-3b20-4681-b03d-28fef4fc0ba6.lance',\n",
       " '../data/lancedb_jina_code\\\\hf_docs.lance\\\\data\\\\ea54ffad-a088-41aa-9709-1b8e46efe2d9.lance',\n",
       " '../data/lancedb_jina_code\\\\hf_docs.lance\\\\_latest.manifest']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "gdown.download_folder(\"https://drive.google.com/drive/folders/1qafKNGxn1SrKIVlCfAwAx0T-eIiKsEYL\",\n",
    "      quiet = True,\n",
    "      output = \"../data/lancedb_jina_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SKkZT-FxX82A",
   "metadata": {
    "id": "SKkZT-FxX82A"
   },
   "source": [
    "If the cell above does not work, create a new folder called `lancedb_jina` in the same directory as this notebook and then download the folder `hf_docs.lance` from https://drive.google.com/drive/folders/1VnioI8X9ZtzCWqJhu5PG-itLKAHszgkI and extract it into the `lancedb_jina` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafx-GeDV9oi",
   "metadata": {
    "id": "aafx-GeDV9oi"
   },
   "source": [
    "# Huggingface Login\n",
    "\n",
    "This homework assignment requires some work with Huggingface Embeddings. Because of this, you must make an account with Huggingface.\n",
    "1. Make an account with Huggingface at https://huggingface.co\n",
    "2. In the top right, click the profile picture and navigate to `Settings`.\n",
    "3. Navigate on the left side-bar to `Access Tokens`.\n",
    "4. Create a token with `Read` access, it can be named whatever you want. Copy this token.\n",
    "5. Run the code below, provide your Huggingf\n",
    "ace access token.\n",
    "6. Navigate to https://huggingface.co/jinaai/jina-embeddings-v2-base-code.\n",
    "7. Accept the terms of use of the embeddings.\n",
    "\n",
    "After this, you should have access to the use of these word embeddings for our LanceDB vectorstore database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fsGJMf3AVeIZ",
   "metadata": {
    "id": "fsGJMf3AVeIZ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd02b41087a414c8ad0d573dd66a809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "# If you struggle with the above, you can try login(token = access_token) where access_token is loaded from a\n",
    "# .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eyFBvlJ7LIG",
   "metadata": {
    "id": "6eyFBvlJ7LIG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Black\\AppData\\Local\\Temp\\ipykernel_11040\\623969373.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_fn = HuggingFaceEmbeddings(model_name = \"jinaai/jina-embeddings-v2-base-code\",\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "from langchain_lancedb.lc_lancedb import LanceDB\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoModel\n",
    "\n",
    "db = lancedb.connect('../data/lancedb_jina_code')\n",
    "\n",
    "# Sets up the embeddings to use the embeddings\n",
    "embedding_fn = HuggingFaceEmbeddings(model_name = \"jinaai/jina-embeddings-v2-base-code\",\n",
    "                                     model_kwargs = {'trust_remote_code' : True})\n",
    "vectorstore = LanceDB(db, embedding_fn, table_name = \"hf_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iDwWbq-Exe0S",
   "metadata": {
    "id": "iDwWbq-Exe0S"
   },
   "source": [
    "# What is the Huggingface Documentation?\n",
    "\n",
    "Huggingface’s documentation has information about many Natural Language Processing tasks. The database is scraped from the website, along with links to the original webpages, and encoded using the jina-embeddings imported above.\n",
    "\n",
    "You can use this database to answer many different questions about course material, which is what we will use this database for later in the assignment. For example, the answer to the question in the code below should mention that a word embedding is a vector to describe the meaning of a word.\n",
    "\n",
    "The vector database is created using a model for word embeddings that was trained on a combination of text and code. These embeddings are the basis of how the vector store database for this assignment operates.\n",
    "\n",
    "This database also contains embeddings of the Huggingface NLP courses, so it contains a wealth of information about NLP.\n",
    "\n",
    "Feel free to explore the database as much as you would like, for this homework assignment and as the course continues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HmXKRLpf9GOJ",
   "metadata": {
    "id": "HmXKRLpf9GOJ"
   },
   "source": [
    "# 1.a. Utilizing LanceDB database to solve queries (5 points)\n",
    "\n",
    "In the function read_database(query), given a string query, use vectorstore.similarity_search(query), which takes as input a string query and returns a list of Documents. Return the page content of the first Document of the list, accessed via the page_content attribute.\n",
    "\n",
    "If you are interested in learning more about how LanceDB operates, there is a description available at https://lancedb.github.io/lancedb/concepts/vector_search/. There are a few graphics there to make it more clear what the database is doing and how it is finding relevant results, given the word embeddings of your query (which are computed automatically by vectorstore.similarity_search(query). There is also some intuition for why we are using vector databases on Slide 11 of the LangChain slides on the course website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "UImrcuyN7ghF",
   "metadata": {
    "id": "UImrcuyN7ghF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/learn/nlp-course/en/chapter1/2\n",
      "Events Join the Hugging Face community and get access to the augmented\n",
      "documentation experience Collaborate on models, datasets and Spaces Faster\n",
      "examples with accelerated inference Switch between documentation themes Sign\n",
      "Up to get started Natural Language Processing Before jumping into Transformer\n",
      "models, let’s do a quick overview of what natural language processing is and\n",
      "why we care about it. What is NLP? NLP is a field of linguistics and machine\n",
      "learning focused on understanding everything related to human language. The\n",
      "aim of NLP tasks is not only to understand single words individually, but to\n",
      "be able to understand the context of those words. The following is a list of\n"
     ]
    }
   ],
   "source": [
    "def read_database(query):\n",
    "  ## YOUR CODE HERE\n",
    "    documents = vectorstore.similarity_search(query) \n",
    "\n",
    "    if documents:\n",
    "        return documents[0].page_content\n",
    "    return ''\n",
    "\n",
    "# This should return the page content for https://huggingface.co/learn/nlp-course/en/chapter1/2\n",
    "print(read_database(\"What is a word embedding?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4dc20e",
   "metadata": {
    "id": "bhNT3QU5G3Jc"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.agents import load_tools\n",
    "import os\n",
    "from tiktoken import encoding_for_model\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Read the local .env file, containing the Open AI secret key.\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "base_url = \"https://api.openai.com/v1/\"\n",
    "model_name = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k90CaYQaEqPS",
   "metadata": {
    "id": "k90CaYQaEqPS"
   },
   "source": [
    "# 1.b. Basic utilization of the LLM with LangChain (10 points)\n",
    "\n",
    "In the function `respond(messages)` take as input a list of tuples `messages`, and create a ChatPromptTemplate from `messages`. `respond(messages)` should return a string containing the LLM response to the conversation.\n",
    "\n",
    "This is very similar to the implementation seen in the second example from the LangChain Jupyter Notebook on the course website.\n",
    "\n",
    "*Hint: ChatPromptTemplate has a method from_messages(listMessages) where listMessages is a list of tuples, with the first element being the role and the second element being the content. This can be given as a prompt to LLMChain. LLMChain can be instantiated with named parameters llm and prompt. It can then be executed with invoke(dict()), since we do not have any formatting in our string, we need to pass an empty dictionary. Remember that invoke returns a dictionary, we want only the string response.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ox31UtTIEpOc",
   "metadata": {
    "id": "ox31UtTIEpOc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space. This technique is used in natural language processing and machine learning tasks to capture semantic relationships between words.\n",
      "\n",
      "Word embeddings are typically learned from large amounts of text data using techniques such as Word2Vec, GloVe, or FastText. These models learn to map words to vectors in such a way that words with similar meanings are located close to each other in the vector space.\n",
      "\n",
      "Word embeddings have been shown to improve the performance of various NLP tasks such as sentiment analysis, named entity recognition, and machine translation. They are also used in recommendation systems, search engines, and other applications that involve processing and understanding human language.\n"
     ]
    }
   ],
   "source": [
    "def respond(messages):\n",
    "    llm = ChatOpenAI(api_key=api_key, base_url=base_url,\n",
    "                    model=model_name, temperature=0, max_tokens=1000)\n",
    "\n",
    "    answer = ''\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    # Create a ChatPromptTemplate.\n",
    "    chat_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    # Create an LLMChain.\n",
    "    chain = LLMChain(llm=llm, prompt=chat_template)\n",
    "\n",
    "    # Get the answer from LLMChain with the invoke() method.\n",
    "    # Remember: LLMChain returns a dictionary, you want to respond with only the string output.\n",
    "    answer = chain.run({})  \n",
    "\n",
    "    return answer\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        'user',\n",
    "        'Tell me what you know about word embeddings.'\n",
    "\n",
    "    )\n",
    "]\n",
    "\n",
    "print(respond(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WrIbBAP_Hwue",
   "metadata": {
    "id": "WrIbBAP_Hwue"
   },
   "source": [
    "# 2. LLMs as Tools (20 + 15 points)\n",
    "\n",
    "While LLMs are used as a component that controls what tools are used, one can also have an LLM with a specific steering prompt as a tool in itself. In this case, this LLM tool will be used as a fact checker. It should be given as input a statement about that topic, and call your `query_database(query)` function to get information. The LLM should determine if the statement follows from the information.\n",
    "\n",
    "This component should return three things:\n",
    "- True or False\n",
    "- An explanation for its answer\n",
    "- The link from the documentation that it referenced.\n",
    "\n",
    "*Hint: LLMs understand Markdown, JSON, and other formatting methods. Utilizing these make it easier for the LLM to understand the difference between different entities in your prompt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "AG_mMhYpuAvl",
   "metadata": {
    "id": "AG_mMhYpuAvl"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "# Create a system message for an LLM that determines if something is true,\n",
    "# given information and a statement about that information.\n",
    "system_message = ( 'system', \"You are an expert in determining if a statement is true or false based on the provided information. \"\n",
    "    \"You will evaluate a statement in the context of the given information and determine its validity.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create instructions containing {information} and {statement} to be used by\n",
    "# the prompt template.\n",
    "instructions = (\n",
    "    \"user\",\n",
    "    \"Given the information: {information}, is the statement '{statement}' true or false? \"\n",
    "    \"Please provide your answer in the following format:\\n\"\n",
    "    \"- Answer: True/False\\n\"\n",
    "    \"- Explanation: [Your explanation here]\\n\"\n",
    "    \"- Reference Link: [Link to the documentation or source used]\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4eb8d",
   "metadata": {},
   "source": [
    "Additionally, replace the \"## YOUR TOOL DESCRIPTION HERE ##\" with a description of what the tool does, and complete the code for the fact_check function such that it gets the response of the model, given the user's message and your system message and instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Q3N0KMrWHps8",
   "metadata": {
    "id": "Q3N0KMrWHps8"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# This decorator defines a tool with the name fact_check\n",
    "@tool\n",
    "def fact_check(statement):\n",
    "    # Place in this string (do not store it anywhere, this describes the tool to the model)\n",
    "    # a description of what this tool does. This will describe to the model\n",
    "    # when to use this tool.\n",
    "    '''\n",
    "    ## This tool is designed to help users answer if a statement is true or false \n",
    "    #  given information from the database. The user provides the statement, \n",
    "    #  and the tool will fetch relevant information and use it to determine the truthfulness of the statement. ##\n",
    "    '''\n",
    "\n",
    "    ## YOUR CODE HERE ##\n",
    "    information = read_database(statement)\n",
    "\n",
    "    # Create a list of tuples in the form (role, content), where role is the speaker\n",
    "    # and content is what is said.\n",
    "\n",
    "    messages = [\n",
    "        ('system', system_message),\n",
    "        ('user', instructions),\n",
    "    ]\n",
    "\n",
    "    return respond(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KU__R3n60-qt",
   "metadata": {
    "id": "KU__R3n60-qt"
   },
   "source": [
    "# 3. Retrievers as Tools (10 points)\n",
    "\n",
    "Vector Stores can be used as tools, referred to as Retrievers. These are tools that are utilized to retrieve data from a source external to the LLM’s parameters. Here we will define a tool to search the Huggingface documentation for relevant information.\n",
    "\n",
    "Replace the text \"## YOUR TOOL DESCRIPTION HERE##\" with a description of the contents of the database and when the LLM should make use of these tools. The database contains information about the documentation of the HuggingFace python module, scraped from their website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2rmwzsPxxFfP",
   "metadata": {
    "id": "2rmwzsPxxFfP"
   },
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Turn the vectorstore into a tool\n",
    "retriever = vectorstore.as_retriever(search_kwargs = dict(k = 1))\n",
    "\n",
    "retriever_tool = create_retriever_tool(retriever, \"search docs\", \"This tool retrieves information from the Hugging Face Python module documentation. It should be used when the user needs detailed information about specific functions, classes, or usage examples within the Hugging Face library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KWhVzmBaN5TG",
   "metadata": {
    "id": "KWhVzmBaN5TG"
   },
   "source": [
    "# 4. Connecting everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grFSBjxzRAAY",
   "metadata": {
    "id": "grFSBjxzRAAY"
   },
   "source": [
    "## 4.a. Set up the agent prompt (40 points)\n",
    "\n",
    "Fill out the system prompt such that it would create an agent that answers questions pertaining to NLP and the Huggingface Transformers library. The prompt provided is a mostly standard ReAct prompt (LangChain slides 17-19), but is formatted to work with LangChain's JSON agent parsing. All you have to do is provide some additional context for how you expect the model to behave by replacing \"## YOUR PROMPT HERE ##\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vydrkLGrQ9du",
   "metadata": {
    "id": "vydrkLGrQ9du"
   },
   "outputs": [],
   "source": [
    "# Produce a system message for this component.\n",
    "\n",
    "system_message = \"\"\"[INST]<<SYS>>\n",
    "Answer questions pertaining to NLP and the Hugging Face Transformers library.\n",
    "<</SYS>>\n",
    "TOOLS:\n",
    "------\n",
    "Where appropriate, Assistant must use tools to access information that is needed to complete User's request. The tools Assistant can use are shown below along with their descriptions:\n",
    "{tools}\n",
    "\n",
    "FORMAT:\n",
    "------\n",
    "To use a tool, Assistant must use the following format:\n",
    "\n",
    "Thought: Do I need to use a tool? Yes\n",
    "```\n",
    "{{\n",
    "\n",
    "  \"action\": $TOOL_NAME, //should be one of [{tool_names}]\n",
    "\n",
    "  \"action_input\": $INPUT //the input to the action\n",
    "\n",
    "}}\n",
    "```\n",
    "Observation: the result of the action\n",
    "\n",
    "When you are ready to respond to the user, use the following format:\n",
    "\n",
    "Thought: Do I need to use a tool? No\n",
    "```\n",
    "  {{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": $FINAL_ANSWER //the final answer to the user's request\n",
    "  }}\n",
    "```\n",
    "<</SYS>>[/INST]\n",
    "\"\"\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "[INST]\n",
    "Conversation history:\n",
    "{chat_history}\n",
    "{input}\n",
    "[/INST]\n",
    "Thought: Do I need to use a tool?{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ONkQl4o6qdB",
   "metadata": {
    "id": "0ONkQl4o6qdB"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Sets up your prompt with few-shot examples for proper formatting.\n",
    "# If you are utilizing Mixtral, combine the system_message and the first HumanMessage.\n",
    "# Mixtral does not have a system message.\n",
    "messages = [\n",
    "      SystemMessagePromptTemplate.from_template(system_message),\n",
    "      HumanMessage(content=\"Before we begin, let's go through some examples.\"),\n",
    "      AIMessage(content=\"I am ready!\"),\n",
    "      HumanMessage(content=\"Hello, I would like to know more about the Huggingface library.\", example=True),\n",
    "      AIMessage(\n",
    "                content=\"Do I need to use a tool? Yes\\n\" \\\n",
    "                \"\\n```\\n{\\\"action\\\": \\\"search docs\\\", \\\"action_input\\\": \\\"Huggingface library\\\"}\\n```\" \\\n",
    "                \"\\nObservation: https://huggingface.co/docs/transformers/index\\nThe Huggingface library is a repository of pre-trained models for Natural Language Processing (NLP) and contains a variety of language models, tokenizers, and other tools for working with NLP and other machine learning tasks.\" \\\n",
    "                \"\\n\\nThought: Do I need to use a tool? No\\n\" \\\n",
    "                \"\\n```\\n{\\\"action\\\": \\\"Final Answer\\\", \\\"action_input\\\": \\\"The Huggingface library is a repository of pre-trained models for Natural Language Processing (NLP) and contains a variety of language models, tokenizers, and other tools for working with NLP and other machine learning tasks. Source: https://huggingface.co/docs/transformers/index\\\"}\\n```\",\n",
    "                example=True\n",
    "                ),\n",
    "      HumanMessage(content=\"Is it true that the Huggingface library hosts audio and vision models too?\", example=True),\n",
    "      AIMessage(\n",
    "                content=\"Do I need to use a tool? Yes\\n\" \\\n",
    "                \"\\n```\\n{\\\"action\\\": \\\"fact_check\\\", \\\"action_input\\\": \\\"The Huggingface library hosts audio and vision models too.\\\"}\\n```\" \\\n",
    "                \"\\nObservation: True. Explanation: The Huggingface library hosts audio, computer vision, and multimodal models in addition to language models and tokenizers. This is true given the information from the following source: https://huggingface.co/docs/transformers/index\" \\\n",
    "                \"\\n\\nThought: Do I need to use a tool? No\\n\" \\\n",
    "                \"\\n```\\n{\\\"action\\\": \\\"Final Answer\\\", \\\"action_input\\\": \\\"True. The Huggingface library hosts audio, computer vision, and multimodal models in addition to language models and tokenizers. This is true given the information from the following source: https://huggingface.co/docs/transformers/index\\\"}\\n```\",\n",
    "                example=True\n",
    "                ),\n",
    "      HumanMessage(content=\"Thank you!\", example=True),\n",
    "      AIMessage(content = \"Do I need to use a tool? No\\n```\\n{\\\"action\\\": \\\"Final Answer\\\", \\\"action_input\\\": \\\"You're welcome!\\\"}\\n```\", example=True),\n",
    "      HumanMessagePromptTemplate.from_template(template=instructions),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NY6l651OW1cQ",
   "metadata": {
    "id": "NY6l651OW1cQ"
   },
   "source": [
    "## 4.b. Set up the agent (20 points)\n",
    "\n",
    "Utilize the `create_json_chat_agent(llm=llm, tools=tools, prompt=prompt)` function of the LangChain library to create an agent to utilize the given prompt. This creation should take place in the `agent_create(context, instructions, tools)` function which takes as it parameters a system messages `system`, instructions to the model (to be put in the user message) `instructions`, and a list of tools `tools`.\n",
    "\n",
    "Pass your agent into the `AgentExecutor` constructor. Notice how limits have been placed to avoid infinite recursion with `max_iterations`, and the ability to remember past turns of the conversation through `ConversationBufferMemory` have been added.\n",
    "\n",
    "*Hint: prompt in create_json_chat_agent needs to be a ChatPromptTemplate.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "uMoA_BUxWXyE",
   "metadata": {
    "id": "uMoA_BUxWXyE"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_json_chat_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def agent_create(messages, tools):\n",
    "    llm = ChatOpenAI(api_key = api_key, base_url = base_url,\n",
    "                 model = model_name, temperature= 0, max_tokens = 1000)\n",
    "\n",
    "    ## YOUR CODE HERE ##\n",
    "    # Produce a ChatPromptTemplate with the instructions.\n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    # Create a JSON agent with the llm, tools, and prompt.\n",
    "    agent = create_json_chat_agent(llm, tools, prompt)\n",
    "    \n",
    "    # Creates an AgentExecutor wrapping around the agent, along with a conversation memory.\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', max_length=4)\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "      agent=agent, tools=tools, verbose=True,\n",
    "      max_iterations=3, handle_parsing_errors=True, memory=memory\n",
    "    )\n",
    "    return agent_executor\n",
    "\n",
    "tools = [retriever_tool, fact_check]\n",
    "\n",
    "# If you want to restart the conversation from scratch, you must run this cell again.\n",
    "agent_executor = agent_create(messages, tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o2srkjGTXKbN",
   "metadata": {
    "id": "o2srkjGTXKbN"
   },
   "source": [
    "# 5.a. Run the agent (20 points)\n",
    "\n",
    "Create an AgentExecutor object containing your agent and tools by defining a function `run_agent(agent, tools)`, then use the `invoke(input)` method to run it. In the markdown cell below **in bold**, record 3 conversation turns with the LangChain assistant. A conversation turn includes both your input and the output from the agent.\n",
    "\n",
    "Note: You will be graded on the quality of your input and your conversation turns, not the output of the model. Ask the model interesting questions, see where it has to utilize the database, when it utilizes the fact checker, or when it is able to respond all on its own. You have free reign to test where your model succeeds and where it fails here.\n",
    "\n",
    "An example of invoking an assistant can be seen in the LangChain examples provided on the course website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "KrsBi47iZbeF",
   "metadata": {
    "id": "KrsBi47iZbeF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "```\n",
      "{\"action\": \"search docs\", \"action_input\": \"Huggingface API options\"}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mhttps://huggingface.co/learn/nlp-course/es/chapter0/1\n",
      "curso de Hugging Face. Esta introducción te guiará en la configuración de un\n",
      "entorno de trabajo. Si acabas de empezar el curso, te recomendamos que primero\n",
      "eches un vistazo al Capítulo 1, y luego vuelvas y configures tu entorno para\n",
      "poder probar el código por ti mismo. Todas las librerías que usaremos en este\n",
      "curso están disponibles como paquetes de Python, así que aquí te mostraremos\n",
      "cómo configurar un entorno de Python e instalar las librerías específicas que\n",
      "necesitarás. Cubriremos dos formas de configurar tu entorno de trabajo,\n",
      "utilizando un cuaderno Colab o un entorno virtual Python. Siéntete libre de\n",
      "elegir la que más te convenga. Para los principiantes, recomendamos\u001b[0m\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "```\n",
      "{\"action\": \"search docs\", \"action_input\": \"Huggingface API options\"}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mhttps://huggingface.co/learn/nlp-course/es/chapter0/1\n",
      "curso de Hugging Face. Esta introducción te guiará en la configuración de un\n",
      "entorno de trabajo. Si acabas de empezar el curso, te recomendamos que primero\n",
      "eches un vistazo al Capítulo 1, y luego vuelvas y configures tu entorno para\n",
      "poder probar el código por ti mismo. Todas las librerías que usaremos en este\n",
      "curso están disponibles como paquetes de Python, así que aquí te mostraremos\n",
      "cómo configurar un entorno de Python e instalar las librerías específicas que\n",
      "necesitarás. Cubriremos dos formas de configurar tu entorno de trabajo,\n",
      "utilizando un cuaderno Colab o un entorno virtual Python. Siéntete libre de\n",
      "elegir la que más te convenga. Para los principiantes, recomendamos\u001b[0m\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "```\n",
      "{\"action\": \"search docs\", \"action_input\": \"Huggingface API options\"}\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3mhttps://huggingface.co/learn/nlp-course/es/chapter0/1\n",
      "curso de Hugging Face. Esta introducción te guiará en la configuración de un\n",
      "entorno de trabajo. Si acabas de empezar el curso, te recomendamos que primero\n",
      "eches un vistazo al Capítulo 1, y luego vuelvas y configures tu entorno para\n",
      "poder probar el código por ti mismo. Todas las librerías que usaremos en este\n",
      "curso están disponibles como paquetes de Python, así que aquí te mostraremos\n",
      "cómo configurar un entorno de Python e instalar las librerías específicas que\n",
      "necesitarás. Cubriremos dos formas de configurar tu entorno de trabajo,\n",
      "utilizando un cuaderno Colab o un entorno virtual Python. Siéntete libre de\n",
      "elegir la que más te convenga. Para los principiantes, recomendamos\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'does hugging offer api options?',\n",
       " 'chat_history': 'Human: briefly explain what hugging face is\\nAI: The Hugging Face library is a repository of pre-trained models for Natural Language Processing (NLP) and contains a variety of language models, tokenizers, and other tools for working with NLP and other machine learning tasks. The Hugging Face library also hosts audio, computer vision, and multimodal models in addition to language models and tokenizers. Source: https://huggingface.co/docs/transformers/index\\nHuman: why is hugging face useful in machine learning?\\nAI: The benefits of the Huggingface library in machine learning include providing a repository of pre-trained models for NLP, hosting audio, computer vision, and multimodal models, and offering tools like Transformers, Datasets, Tokenizers, and Accelerate for NLP tasks. The library allows users to work with Transformer models, fine-tune them on datasets, share results on the Hugging Face Hub, and optimize models for production environments. It also covers the basics of NLP tasks, such as using Datasets and Tokenizers, and delves into processing tasks in speech and computer vision using Transformer models. The course requires good Python knowledge and familiarity with deep learning concepts, but prior knowledge of PyTorch or TensorFlow is not necessary. Completing the course prepares users to apply Huggingface Transformers to various machine learning problems.',\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_agent(agent, tools):\n",
    "    inputDict = dict()\n",
    "\n",
    "    userInput = input('User: ')\n",
    "    inputDict = {'input': userInput}\n",
    "\n",
    "    ## YOUR CODE HERE ##\n",
    "    # Return the output of the agent executor for the given user input.\n",
    "    return agent_executor.invoke(inputDict)\n",
    "\n",
    "run_agent(agent_executor, tools)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "itcs-3156",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
